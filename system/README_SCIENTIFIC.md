# LightweightPeptideGen: A GAN-Based Framework for Conditional Generation of Structurally Stable Antimicrobial Peptides

> **B√°o c√°o k·ªπ thu·∫≠t / Technical Report** ‚Äî v1.0, February 2026

---

## T√≥m t·∫Øt (Abstract)

Ch√∫ng t√¥i tr√¨nh b√†y **LightweightPeptideGen**, m·ªôt framework h·ªçc s√¢u d·ª±a tr√™n Generative Adversarial Network (GAN) ƒë·ªÉ sinh t·ª± ƒë·ªông c√°c **antimicrobial peptide (AMP)** c√≥ ƒë·ªô ·ªïn ƒë·ªãnh c·∫•u tr√∫c cao. Framework t√≠ch h·ª£p nhi·ªÅu c∆° ch·∫ø ti√™n ti·∫øn: (1) b·ªô sinh chu·ªói t·ª± h·ªìi quy GRU/LSTM/Transformer v·ªõi ƒëi·ªÅu ki·ªán h√≥a ƒëa t√≠nh nƒÉng, (2) b·ªô ph√¢n bi·ªát CNN ƒëa kernel k·∫øt h·ª£p Spectral Normalization v√† Minibatch Discrimination, (3) t√≠ch h·ª£p m√¥ h√¨nh ng√¥n ng·ªØ protein ESM2 (650M tham s·ªë) l√†m ƒë√°nh gi√° vi√™n c·∫•u tr√∫c k·∫øt h·ª£p v·ªõi Graph Attention Network (GAT), (4) h·ªá th·ªëng loss ƒëa th√†nh ph·∫ßn ch·ªëng mode collapse g·ªìm Diversity Loss, N-gram Diversity Loss, Length Penalty Loss v√† Feature Matching Loss. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n **129,121 chu·ªói peptide** v·ªõi ƒëi·ªÅu ki·ªán h√≥a 8 ƒë·∫∑c tr∆∞ng sinh h√≥a, ƒë·∫°t G validation loss ‚âà 0.407, entropy > 2.89, D‚ÄìG gap < 0.25 sau h∆°n 200 epoch.

---

## M·ª•c l·ª•c

1. [Gi·ªõi thi·ªáu v√† B·ªëi c·∫£nh](#1-gi·ªõi-thi·ªáu)
2. [Ki·∫øn tr√∫c H·ªá th·ªëng](#2-ki·∫øn-tr√∫c-h·ªá-th·ªëng)
3. [C√°c K·ªπ thu·∫≠t Ch√≠nh](#3-c√°c-k·ªπ-thu·∫≠t-ch√≠nh)
4. [H√†m M·∫•t m√°t](#4-h√†m-m·∫•t-m√°t)
5. [D·ªØ li·ªáu v√† Dataset](#5-d·ªØ-li·ªáu-v√†-dataset)
6. [ƒê·∫∑c tr∆∞ng ƒêi·ªÅu ki·ªán H√≥a](#6-ƒë·∫∑c-tr∆∞ng-ƒëi·ªÅu-ki·ªán-h√≥a)
7. [ƒê√°nh gi√° v√† Ch·ªâ s·ªë Ch·∫•t l∆∞·ª£ng](#7-ƒë√°nh-gi√°-v√†-ch·ªâ-s·ªë-ch·∫•t-l∆∞·ª£ng)
8. [Y√™u c·∫ßu Ph·∫ßn c·ª©ng & GPU](#8-y√™u-c·∫ßu-ph·∫ßn-c·ª©ng--gpu)
9. [Th∆∞ vi·ªán v√† Dependencies](#9-th∆∞-vi·ªán-v√†-dependencies)
10. [C·∫•u h√¨nh Si√™u tham s·ªë](#10-c·∫•u-h√¨nh-si√™u-tham-s·ªë)
11. [K·∫øt qu·∫£ Th·ª±c nghi·ªám](#11-k·∫øt-qu·∫£-th·ª±c-nghi·ªám)
12. [C·∫•u tr√∫c D·ª± √°n](#12-c·∫•u-tr√∫c-d·ª±-√°n)
13. [H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng](#13-h∆∞·ªõng-d·∫´n-s·ª≠-d·ª•ng)
14. [T√†i li·ªáu Tham kh·∫£o](#14-t√†i-li·ªáu-tham-kh·∫£o)

---

## 1. Gi·ªõi thi·ªáu

### 1.1 V·∫•n ƒë·ªÅ nghi√™n c·ª©u

Antimicrobial peptides (AMPs) l√† c√°c ph√¢n t·ª≠ peptide ng·∫Øn (th∆∞·ªùng 5‚Äì50 amino acid) c√≥ kh·∫£ nƒÉng ti√™u di·ªát vi khu·∫©n kh√°ng thu·ªëc. Vi·ªác thi·∫øt k·∫ø AMP theo ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng (mutagenesis, s√†ng l·ªçc th·ª±c nghi·ªám) t·ªën k√©m v√† kh√¥ng c√≥ kh·∫£ nƒÉng m·ªü r·ªông. C√°c ph∆∞∆°ng ph√°p h·ªçc s√¢u sinh t·∫°o (generative deep learning) m·ªü ra kh·∫£ nƒÉng kh√°m ph√° kh√¥ng gian chu·ªói r·ªông l·ªõn m·ªôt c√°ch hi·ªáu qu·∫£.

### 1.2 Th√°ch th·ª©c

- **Mode collapse**: GAN c√≥ xu h∆∞·ªõng sinh ra c√°c chu·ªói l·∫∑p ƒëi l·∫∑p l·∫°i
- **Length collapse**: M√¥ h√¨nh k·∫øt th√∫c chu·ªói qu√° s·ªõm ho·∫∑c qu√° mu·ªôn
- **ƒê·ªô ·ªïn ƒë·ªãnh sinh h√≥a**: C·∫ßn ki·ªÉm so√°t Instability Index, GRAVY, Aliphatic Index
- **T√≠nh ƒëa d·∫°ng vs. ch·∫•t l∆∞·ª£ng**: Trade-off gi·ªØa novelty v√† t√≠nh h·ª£p l·ªá sinh h·ªçc

### 1.3 ƒê√≥ng g√≥p ch√≠nh

1. Framework GAN v·ªõi h·ªá th·ªëng loss ƒëa th√†nh ph·∫ßn chuy√™n bi·ªát cho peptide
2. T√≠ch h·ª£p ESM2 + GAT l√†m structural evaluator trong v√≤ng l·∫∑p hu·∫•n luy·ªán
3. C∆° ch·∫ø ƒëi·ªÅu ki·ªán h√≥a 8 ƒë·∫∑c tr∆∞ng sinh h√≥a cho ph√©p ki·ªÉm so√°t t√≠nh ch·∫•t
4. Adaptive Discriminator Training ch·ªëng D dominance
5. N-gram v√† Length Penalty Loss gi·∫£i quy·∫øt motif collapse v√† length collapse

---

## 2. Ki·∫øn tr√∫c H·ªá th·ªëng

### 2.1 T·ªïng quan ki·∫øn tr√∫c GAN

```
Latent Vector z ~ N(0, I)          Condition Vector c (8-dim)
         ‚îÇ                                     ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ   GENERATOR G   ‚îÇ  GRU/LSTM/Transformer
               ‚îÇ  (autoregressive‚îÇ  + Self-Attention
               ‚îÇ   decoding)     ‚îÇ  + Condition Fusion
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ  Soft one-hot logits (B, L, V)
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚Üì                    ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  DISCRIMINATOR D  ‚îÇ   ‚îÇ  STRUCTURE EVAL      ‚îÇ
   ‚îÇ  CNN multi-kernel ‚îÇ   ‚îÇ  ESM2-650M + GAT     ‚îÇ
   ‚îÇ  + Spectral Norm  ‚îÇ   ‚îÇ  (stability scoring) ‚îÇ
   ‚îÇ  + Minibatch Std  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Generator ‚Äî `GRUGenerator`

**File:** `peptidegen/models/generator.py`

B·ªô sinh ch√≠nh l√† `GRUGenerator`, k·∫ø th·ª´a t·ª´ base class `PeptideGenerator`:

```
Input:
  z  ‚àà ‚Ñù^{B √ó latent_dim}       # Latent noise vector
  c  ‚àà ‚Ñù^{B √ó condition_dim}    # Condition features (8 physicochemical)

Architecture:
  1. Latent projection: MLP(z ‚äï c) ‚Üí h‚ÇÄ ‚àà ‚Ñù^{B √ó hidden_dim}
  2. Embedding: AA token ‚Üí ‚Ñù^{embedding_dim}
  3. GRU (num_layers=2, hidden_dim=256, bidirectional=True*)
     *bidirectional for resumed checkpoints; unidirectional for new runs
  4. SelfAttention (attention head over GRU outputs)
  5. Output projection: Linear ‚Üí ‚Ñù^{vocab_size=24}

Output: logits ‚àà ‚Ñù^{B √ó L √ó 24}  (soft one-hot distribution)

Decoding modes:
  - Teacher forcing: during training (target provided)
  - Autoregressive: during inference (token-by-token)
  - Sampling: temperature, top-k, top-p (nucleus sampling)
```

**C√°c bi·∫øn th·ªÉ Generator:**

| Model | Tham s·ªë | ƒê·∫∑c ƒëi·ªÉm |
|---|---|---|
| `GRUGenerator` | ~2.4M | Autoregressive GRU + SelfAttention, gradient checkpointing |
| `LSTMGenerator` | ~2.5M | LSTM v·ªõi cell state + condition fusion |
| `TransformerGenerator` | ~3M | Multi-head attention, positional encoding |
| `ESM2ConditionedGenerator` | ~10M+ | ƒêi·ªÅu ki·ªán h√≥a b·∫±ng ESM2 embedding |

**K·ªπ thu·∫≠t quan tr·ªçng trong Generator:**
- **Gradient Checkpointing**: `use_gradient_checkpointing=True` ƒë·ªÉ gi·∫£m VRAM khi hidden_dim l·ªõn
- **Condition Fusion**: concat(z, c) ‚Üí Linear ‚Üí tanh cho hidden state ban ƒë·∫ßu
- **Autoregressive sampling**: ch·ªçn token t·∫°i m·ªói b∆∞·ªõc theo ph√¢n ph·ªëi softmax(logits/T)

### 2.3 Discriminator ‚Äî `CNNDiscriminator`

**File:** `peptidegen/models/discriminator.py`

```
Input: x ‚àà ‚Ñù^{B √ó L} (token indices) ho·∫∑c ‚Ñù^{B √ó L √ó V} (soft logits)

Architecture:
  1. Embedding: token ‚Üí ‚Ñù^{embedding_dim}
  2. Parallele 1D-Conv branches (TextCNN style):
     - Conv1D(in=embedding_dim, out=64,  kernel=3) + ReLU + MaxPool
     - Conv1D(in=embedding_dim, out=128, kernel=5) + ReLU + MaxPool
     - Conv1D(in=embedding_dim, out=256, kernel=7) + ReLU + MaxPool
  3. Feature concatenation: [64 + 128 + 256] = 448-dim
  4. Minibatch Standard Deviation (anti mode-collapse)
  5. MLP classifier: 448+1 ‚Üí 256 ‚Üí 1 (logit score)

Regularization:
  - Spectral Normalization tr√™n t·∫•t c·∫£ Linear/Conv layers
  - Label Smoothing (0.15 cho real labels)
  - Instance Noise (œÉ=0.08) th√™m v√†o input D

Output: score ‚àà ‚Ñù^{B √ó 1}
```

**C√°c bi·∫øn th·ªÉ Discriminator:**

| Model | ƒê·∫∑c ƒëi·ªÉm |
|---|---|
| `CNNDiscriminator` | Multi-kernel TextCNN, Spectral Norm, Minibatch Std |
| `RNNDiscriminator` | Bidirectional GRU + Attention, Spectral Norm |
| `HybridDiscriminator` | CNN + RNN k·∫øt h·ª£p, t·ªïng h·ª£p hai lu·ªìng feature |

---

## 3. C√°c K·ªπ thu·∫≠t Ch√≠nh

### 3.1 ESM2 Integration ‚Äî Protein Language Model

**File:** `peptidegen/models/esm2_embedder.py`

ESM2 (Evolutionary Scale Modeling 2) l√† m√¥ h√¨nh ng√¥n ng·ªØ protein ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n 250M protein sequences t·ª´ UniRef50 database.

```
ESM2Embedder:
  - Model: esm2_t33_650M_UR50D (33 layers, 1280-dim, 650M params)
  - Weights: FROZEN (freeze_esm=True) ‚Äî ch·ªâ d√πng l√†m feature extractor
  - Pooling: mean pooling qua token dimension ‚Üí ‚Ñù^{1280}
  - Projection: LightweightESMProjector
      Linear(1280 ‚Üí 640) + GELU + Dropout + Linear(640 ‚Üí 128) + LayerNorm

S·ª≠ d·ª•ng:
  sequences: List[str] ‚Üí ESM2 tokenize ‚Üí forward ‚Üí mean pool ‚Üí project
  ‚Üí embedding ‚àà ‚Ñù^{B √ó 128}
```

**B·∫£ng ESM2 Model Variants:**

| Model | Layers | Params | Embed Dim | GPU RAM |
|---|---|---|---|---|
| `esm2_t6_8M_UR50D` | 6 | 8M | 320 | ~2 GB |
| `esm2_t12_35M_UR50D` | 12 | 35M | 480 | ~4 GB |
| `esm2_t30_150M_UR50D` | 30 | 150M | 640 | ~8 GB |
| `esm2_t33_650M_UR50D` | 33 | **650M** | **1280** | **~16 GB** |
| `esm2_t36_3B_UR50D` | 36 | 3B | 2560 | ~40 GB |

### 3.2 Graph Attention Network (GAT) ‚Äî Structure Evaluator

**File:** `peptidegen/models/structure_evaluator.py`

GAT m√¥ h√¨nh h√≥a peptide nh∆∞ m·ªôt **ƒë·ªì th·ªã tuy·∫øn t√≠nh** trong ƒë√≥ m·ªói amino acid l√† m·ªôt node, c√°c c·∫°nh k·∫øt n·ªëi c√°c residue l√¢n c·∫≠n trong c·ª≠a s·ªï sliding window.

```
GraphAttentionLayer:
  Input: node features h ‚àà ‚Ñù^{B √ó N √ó d_in}
         adjacency matrix A ‚àà {0,1}^{B √ó N √ó N}  (window_size=3)

  Attention coefficient:
    e_ij = LeakyReLU(a^T [Wh_i ‚Äñ Wh_j])  (LeakyReLU Œ±=0.2)
    Œ±_ij = softmax_j(e_ij)               (masked by A)

  Update:
    h'_i = œÉ(Œ£_j Œ±_ij ¬∑ Wh_j)

  Multi-head: concat K heads ‚Üí projection

LightweightGAT:
  num_layers: 2
  num_heads: 4
  hidden_dim: 64
  output_dim: 32

ESM2StructureEvaluator:
  ESM2Embedder ‚Üí token embeddings (B, L, 1280)
  ‚Üí Project (B, L, 128)
  ‚Üí LightweightGAT (B, L, 32)
  ‚Üí Global mean pool (B, 32)
  ‚Üí MLP ‚Üí stability_score ‚àà ‚Ñù^{B √ó 1}
```

### 3.3 Minibatch Discrimination

**File:** `peptidegen/models/discriminator.py` ‚Äî `CNNDiscriminator`

K·ªπ thu·∫≠t ch·ªëng mode collapse: t√≠nh **Minibatch Standard Deviation** ƒë·ªÉ cung c·∫•p th√¥ng tin v·ªÅ diversity c·ªßa batch cho Discriminator.

```python
# T√≠nh std theo batch dimension, append v√†o feature map
minibatch_std = x.std(dim=0, keepdim=True).mean().expand(B, 1)
x = torch.cat([x, minibatch_std], dim=-1)
# ‚Üí feature dim: 448 ‚Üí 449
```

### 3.4 Spectral Normalization

√Åp d·ª•ng cho **t·∫•t c·∫£ Linear v√† Conv layers** trong Discriminator ƒë·ªÉ r√†ng bu·ªôc Lipschitz constant v·ªÅ 1, ·ªïn ƒë·ªãnh training GAN:

```python
nn.utils.spectral_norm(nn.Linear(in, out))
nn.utils.spectral_norm(nn.Conv1d(in, out, k))
```

### 3.5 Adaptive Discriminator Training

**File:** `peptidegen/training/trainer.py` ‚Äî `GANTrainer`

```
M·ªói training step:
  gap = D_real_loss - D_fake_loss

  N·∫øu gap > d_threshold (2.0):
    ‚Üí Skip D update (D qu√° m·∫°nh)
    ‚Üí TƒÉng G steps l√™n g_steps √ó g_steps_boost

  N·∫øu gap ‚â§ d_threshold:
    ‚Üí C·∫≠p nh·∫≠t D b√¨nh th∆∞·ªùng (d_steps=1)
    ‚Üí C·∫≠p nh·∫≠t G (g_steps=8)
```

### 3.6 Mixed Precision Training (AMP)

S·ª≠ d·ª•ng `torch.cuda.amp` v·ªõi `GradScaler` ƒë·ªÉ:
- Gi·∫£m 50% memory usage (float16 thay float32)
- TƒÉng t·ªëc ~2x tr√™n Tensor Core GPUs
- `use_amp=True` trong config

### 3.7 Conditional Generation

**File:** `peptidegen/training/trainer.py` ‚Äî `ConditionalGANTrainer`

```
Condition vector c ‚àà ‚Ñù^{B √ó 8}  (8 physicochemical features)

Fusion trong Generator:
  z_fused = concat(z, c)          # ‚Ñù^{B √ó (latent_dim + condition_dim)}
  h‚ÇÄ = tanh(Linear(z_fused))     # Initial hidden state

ConditionalGANTrainer:
  Inherits GANTrainer
  Adds: feature_loss_weight=0.1
  Passes conditions tensor qua _generate()
```

---

## 4. H√†m M·∫•t m√°t

**File:** `peptidegen/training/losses.py`

T·ªïng loss c·ªßa Generator:

```
L_G = w_adv ¬∑ L_adversarial
    + w_rec ¬∑ L_reconstruction
    + w_div ¬∑ L_diversity
    + w_ngram ¬∑ L_ngram
    + w_len ¬∑ L_length_penalty
    + w_fm ¬∑ L_feature_matching
    + w_stab ¬∑ L_stability_bias
```

### 4.1 Adversarial Loss

**Binary Cross Entropy** v·ªõi label smoothing:

```
L_adv_D = BCE(D(x_real), smooth_real) + BCE(D(x_fake), 0)
L_adv_G = BCE(D(x_fake), 1)

smooth_real = 1 - label_smoothing = 0.85
```

Noise injection v√†o D input: `x_noisy = x + N(0, noise_std¬≤)` v·ªõi `noise_std=0.08`

### 4.2 Diversity Loss ‚Äî `DiversityLoss`

Ba th√†nh ph·∫ßn ch·ªëng mode collapse:

```
1. Token Entropy (per position):
   H_pos = -Œ£_v p_v ¬∑ log(p_v + Œµ)    over vocab dimension
   L_entropy = -mean(H_pos)             ‚Üí minimize ‚Üí maximize entropy

2. Batch Similarity:
   probs_mean = mean(softmax(logits), dim=0)   # Mean distribution
   L_batch_sim = cosine_sim(probs_i, probs_mean)  ‚Üí penalize similarity

3. Pairwise Distance:
   Sample pairs (i,j) from batch
   L_pairwise = ReLU(margin - ||logits_i - logits_j||¬≤)
   margin = 1.0

L_diversity = w_ent¬∑H + w_batch¬∑L_batch_sim + w_pair¬∑L_pairwise
weights: (0.3, 0.3, 0.4)
Total weight in training: diversity_weight = 1.4
```

### 4.3 N-gram Diversity Loss ‚Äî `NgramDiversityLoss`

Ph·∫°t c√°c motif l·∫∑p l·∫°i ·ªü c·∫•p ƒë·ªô bigram v√† trigram:

```
Bigram joint distribution:
  p(a,b) = Œ£_t softmax(logits[:,t,:]) ‚äó softmax(logits[:,t+1,:])
  H_bigram = -Œ£ p(a,b) ¬∑ log(p(a,b) + Œµ)

Trigram: t∆∞∆°ng t·ª± v·ªõi t, t+1, t+2

L_ngram = bigram_weight ¬∑ (1 - H_bigram/log(V¬≤))
        + trigram_weight ¬∑ (1 - H_trigram/log(V¬≥))
weights: (0.5, 0.5)
Total weight: ngram_weight = 0.45
```

### 4.4 Length Penalty Loss ‚Äî `LengthPenaltyLoss`

Gi·∫£i quy·∫øt length collapse b·∫±ng EOS cumulative probability supervision:

```
eos_prob_t = softmax(logits[:,t,:])[eos_idx]       # P(EOS at position t)
cum_eos_t  = 1 - Œ†_{s‚â§t} (1 - eos_prob_s)         # P(EOS by position t)

Target:
  t < target_min (=10): cum_eos_t should be ‚âà 0
  t > target_max (=30): cum_eos_t should be ‚âà 1

L_early = Œ£_{t<10}  early_weight ¬∑ cum_eos_t¬≤
L_late  = Œ£_{t>30}  late_weight  ¬∑ (1 - cum_eos_t)¬≤
L_length = (L_early + L_late) / seq_len
Total weight: length_penalty_weight = 0.5
```

### 4.5 Feature Matching Loss ‚Äî `FeatureMatchingLoss`

·ªîn ƒë·ªãnh training b·∫±ng c√°ch kh·ªõp intermediate features gi·ªØa real v√† fake:

```
L_fm = ||E[D_feat(x_real)] - E[D_feat(x_fake)]||¬≤‚ÇÇ

L·∫•y features t·ª´ CNNDiscriminator.get_feature() tr∆∞·ªõc output layer
Total weight: feature_matching_weight = 0.2
```

### 4.6 Reconstruction Loss ‚Äî `ReconstructionLoss`

```
L_rec = CrossEntropy(logits, real_targets, ignore_index=PAD)
Ho·∫∑c MSE v·ªõi soft one-hot targets (reconstruction_loss="mse")
Total weight: reconstruction_weight = 0.3
```

### 4.7 Stability Bias Loss ‚Äî `StabilityBiasLoss`

Soft nudge h∆∞·ªõng ƒë·∫øn Instability Index < 30:

```
II_proxy = heuristic_instability_score(generated_tokens)
L_stab = ReLU(II_proxy - target_stability_ii) / target_stability_ii
Total weight: stability_weight = 0.1
```

---

## 5. D·ªØ li·ªáu v√† Dataset

### 5.1 Th·ªëng k√™ Dataset

| Split | Sequences | File |
|---|---|---|
| Train | **129,121** | `dataset/train.fasta` + `train.csv` |
| Validation | **27,669** | `dataset/val.fasta` + `val.csv` |
| Test | ‚Äî | `dataset/test.fasta` + `test.csv` |
| **T·ªïng** | **~156,790** | |

### 5.2 Format D·ªØ li·ªáu

- **FASTA files**: chu·ªói amino acid raw sequence
- **CSV files**: k√®m 8 c·ªôt ƒë·∫∑c tr∆∞ng sinh h√≥a (xem Section 6)
- **Vocabulary**: 24 tokens = 20 standard amino acids + `<PAD>`, `<SOS>`, `<EOS>`, `<UNK>`
- **ƒê·ªô d√†i**: min=5, max=50 residues

### 5.3 Data Pipeline

**Files:** `peptidegen/data/`

```
dataset.py:
  PeptideDataset          ‚Äî FASTA-only loader
  ConditionalPeptideDataset ‚Äî CSV + FASTA loader v·ªõi 8-dim condition

dataloader.py:
  create_dataloader()     ‚Äî batch_size=8192, num_workers=8, pin_memory=True

features.py:
  FeatureExtractor        ‚Äî T√≠nh 8 ƒë·∫∑c tr∆∞ng t·ª´ chu·ªói amino acid
  Normalization           ‚Äî z-score normalization c·ªßa features

vocabulary.py:
  VOCAB                   ‚Äî Global singleton vocab object
  encode(seq) / decode(ids)
```

---

## 6. ƒê·∫∑c tr∆∞ng ƒêi·ªÅu ki·ªán H√≥a

8 ƒë·∫∑c tr∆∞ng sinh h√≥a d√πng ƒë·ªÉ ƒëi·ªÅu ki·ªán h√≥a generation:

| Feature | M√¥ t·∫£ | Ng∆∞·ª°ng/√ù nghƒ©a | File t√≠nh |
|---|---|---|---|
| `instability_index` | Ch·ªâ s·ªë b·∫•t ·ªïn ƒë·ªãnh (Guruprasad 1990) | < 40 = stable | `stability.py` |
| `therapeutic_score` | ƒêi·ªÉm ti·ªÅm nƒÉng ƒëi·ªÅu tr·ªã (0‚Äì10) | > 7 = high | `metrics.py` |
| `hemolytic_score` | ƒê·ªô ƒë·ªôc t·∫ø b√†o h·ªìng c·∫ßu (0‚Äì10) | < 3 = safe | `metrics.py` |
| `aliphatic_index` | Ch·ªâ s·ªë aliphatic (Ikai 1980) | Cao = b·ªÅn nhi·ªát | `stability.py` |
| `hydrophobic_moment` | Amphipathicity (Eisenberg scale) | Cao ‚Üí AMP t·ªët | `metrics.py` |
| `gravy` | Grand Average Hydropathicity | < 0 = hydrophilic | `stability.py` |
| `charge_at_pH7` | ƒêi·ªán t√≠ch th·ª±c t·∫°i pH 7.0 | > 0 = cationic AMP | `stability.py` |
| `aromaticity` | T·ªâ l·ªá Phe/Trp/Tyr | ‚Äî | `stability.py` |

---

## 7. ƒê√°nh gi√° v√† Ch·ªâ s·ªë Ch·∫•t l∆∞·ª£ng

**Files:** `peptidegen/evaluation/`

### 7.1 Training Metrics (theo d√µi m·ªói epoch)

| Metric | K√Ω hi·ªáu | M·ª•c ti√™u | C√¥ng th·ª©c |
|---|---|---|---|
| G Validation Loss | `val_g_loss` | **< 0.35** | BCE(D(G(z)), 1) |
| D‚ÄìG Gap | `gap` | **< 0.25** | D_real_loss ‚àí D_fake_loss |
| Token Entropy | `ent` | **> 2.80** | ‚àíŒ£ p log p (per token) |
| N-gram Diversity | `ngram` | **> 0.08** | Bigram/Trigram joint entropy |
| Length Penalty | `len_pen` | **< 0.005** | EOS cumulative prob loss |
| Stability Loss | `stab` | minimize | ReLU(II ‚àí 30) |

### 7.2 Sequence Quality Metrics

**File:** `peptidegen/evaluation/stability.py`

```python
calculate_instability_index(seq)   # Guruprasad DIWV dipeptide weights
calculate_gravy(seq)                # Kyte-Doolittle hydropathy
calculate_aliphatic_index(seq)     # Ikai: 100√ó(xA + 2.9√óxV + 3.9√ó(xI+xL))
calculate_isoelectric_point(seq)   # Bisection method, pKa table
calculate_charge_at_pH(seq, pH=7)  # Henderson-Hasselbalch
calculate_molecular_weight(seq)    # Residue MW table
calculate_aromaticity(seq)         # freq(F) + freq(W) + freq(Y)
calculate_secondary_structure_propensity(seq)  # Helix/Sheet Chou-Fasman
```

### 7.3 AMP-specific Metrics

**File:** `peptidegen/evaluation/metrics.py`

```python
calculate_hydrophobicity(seq)      # Eisenberg scale
calculate_hydrophobic_moment(seq)  # Œ±-helix angle 100¬∞, window=11
calculate_net_charge(seq)          # pH-aware charge
calculate_hemolytic_score(seq)     # Hemolytic propensity scale
calculate_therapeutic_score(seq)   # Composite: charge + hydrophobicity + amphipathicity
estimate_amp_probability(seq)      # Heuristic ML-based AMP probability
analyze_amp_properties(seqs)       # Batch analysis v·ªõi summary statistics
```

### 7.4 Diversity Metrics

```python
calculate_diversity_metrics(seqs):
  - uniqueness_ratio     = |unique_seqs| / |total_seqs|
  - ngram_diversity_2    = |unique_bigrams| / |total_bigrams|
  - ngram_diversity_3    = |unique_trigrams| / |total_trigrams|
  - avg_levenshtein      = mean pairwise edit distance (sampled)
  - entropy_aa           = Shannon entropy of AA distribution

detect_mode_collapse(seqs):
  - entropy_threshold: 0.3  (entropy < threshold ‚Üí collapse)
  - aa_usage_threshold: 0.5 (fraction of AAs used < threshold ‚Üí collapse)
  - repetition_ratio: fraction of duplicated subsequences
```

### 7.5 Quality Filter

**File:** `peptidegen/evaluation/quality_filter.py`

Filter pipeline sau khi sinh:
```
1. ƒê·ªô d√†i: 10 ‚â§ len ‚â§ 30
2. Instability Index < 40 (stable)
3. min_stability_score ‚â• 0.5
4. Ch·ªâ ch·ª©a 20 standard amino acids
5. Kh√¥ng c√≥ run qu√° d√†i (> 5 k√Ω t·ª± gi·ªëng nhau li√™n ti·∫øp)
```

---

## 8. Y√™u c·∫ßu Ph·∫ßn c·ª©ng & GPU

### 8.1 GPU th·ª±c t·∫ø s·ª≠ d·ª•ng

```
GPU:     NVIDIA Quadro RTX 6000
VRAM:    23.5 GB GDDR6
CUDA:    12.x
Batch:   8,192 sequences/batch
Speed:   ~44 gi√¢y/epoch (15 batches √ó 8192 = 122,880 seqs)
```

### 8.2 Y√™u c·∫ßu t·ªëi thi·ªÉu

| C·∫•u h√¨nh | GPU | VRAM | Batch | ESM2 Model |
|---|---|---|---|---|
| **Minimum** | GTX 1080 Ti | 11 GB | 512 | esm2_t6_8M (8M) |
| **Recommended** | RTX 3090 | 24 GB | 4096 | esm2_t12_35M |
| **Optimal** | RTX 3090 / A100 | 24‚Äì40 GB | 8192 | esm2_t33_650M |
| **Production** | A100 80GB | 80 GB | 16384 | esm2_t36_3B |

### 8.3 T·ªëi ∆∞u Memory

| K·ªπ thu·∫≠t | C·∫•u h√¨nh | Ti·∫øt ki·ªám |
|---|---|---|
| Mixed Precision (AMP) | `use_amp: true` | ~50% VRAM |
| Gradient Checkpointing | `gradient_checkpointing: true` | ~30% VRAM |
| ESM2 Frozen | `freeze_esm: true` | Kh√¥ng train 650M params |
| Pin Memory | `pin_memory: true` | TƒÉng PCIe bandwidth |
| Num Workers | `num_workers: 8` | TƒÉng CPU‚ÜíGPU throughput |

---

## 9. Th∆∞ vi·ªán v√† Dependencies

### 9.1 Core Dependencies

| Th∆∞ vi·ªán | Phi√™n b·∫£n | Vai tr√≤ |
|---|---|---|
| **PyTorch** | ‚â• 2.0.0 | Deep learning framework ch√≠nh |
| **fair-esm** | ‚â• 2.0.0 | ESM2 protein language model (Facebook AI) |
| **torch-geometric** | ‚â• 2.3.0 | Graph Attention Network (GAT) layers |
| **NumPy** | ‚â• 1.24.0 | Numerical computing |
| **SciPy** | ‚â• 1.10.0 | Scientific computing, optimization |
| **Pandas** | ‚â• 2.0.0 | CSV data loading v√† x·ª≠ l√Ω |
| **PyYAML** | ‚â• 6.0 | Configuration file parsing |
| **tqdm** | ‚â• 4.65.0 | Progress bars |

### 9.2 Optional Dependencies

| Th∆∞ vi·ªán | Vai tr√≤ |
|---|---|
| matplotlib ‚â• 3.7 | Visualization c·ªßa training curves |
| seaborn ‚â• 0.12 | Statistical plots |
| biopython ‚â• 1.81 | Advanced bioinformatics analysis |
| pytest ‚â• 7.3 | Unit testing |

### 9.3 C√†i ƒë·∫∑t

```bash
# Conda environment (khuy·∫øn ngh·ªã)
conda env create -f environment.yml
conda activate peptidegen

# pip
pip install -r requirements.txt

# ESM2 (n·∫øu c·∫ßn t·∫£i model separately)
python -c "import esm; esm.pretrained.esm2_t33_650M_UR50D()"
```

---

## 10. C·∫•u h√¨nh Si√™u tham s·ªë

### 10.1 Ki·∫øn tr√∫c Model

| Tham s·ªë | Gi√° tr·ªã | M√¥ t·∫£ |
|---|---|---|
| `latent_dim` | 128 | Chi·ªÅu noise vector z |
| `embedding_dim` | 128 | Chi·ªÅu AA embedding (config m·ªõi: 128, checkpoint c≈©: 64) |
| `hidden_dim` | 512 | Chi·ªÅu ·∫©n GRU (config m·ªõi: 512, checkpoint c≈©: 256) |
| `num_layers` | 2 | S·ªë GRU layers |
| `dropout` | 0.2 | Dropout rate |
| `condition_dim` | 8 | Chi·ªÅu condition vector (8 physicochemical features) |
| `vocab_size` | 24 | 20 AA + 4 special tokens |
| `max_seq_length` | 50 | Max peptide length |

### 10.2 Training Hyperparameters

| Tham s·ªë | Gi√° tr·ªã | M√¥ t·∫£ |
|---|---|---|
| `batch_size` | 8192 | Sequences per batch |
| `learning_rate` | 3e-5 | Generator LR (Adam) |
| `lr_discriminator` | 1e-5 | Discriminator LR (Adam) |
| `beta1` | 0.5 | Adam Œ≤‚ÇÅ |
| `beta2` | 0.999 | Adam Œ≤‚ÇÇ |
| `weight_decay` | 1e-4 | L2 regularization |
| `grad_clip` | 1.0 | Gradient clipping |
| `g_steps` | 8 | G updates per D update |
| `d_steps` | 1 | D updates per iteration |
| `label_smoothing` | 0.15 | Real label smoothing |
| `noise_std` | 0.08 | Instance noise trong D |
| `d_threshold` | 2.0 | D skip threshold (adaptive) |
| `patience` | 50 | Early stopping patience |

### 10.3 Loss Weights

| Loss | Weight | Ch√∫ th√≠ch |
|---|---|---|
| `adversarial_weight` | 0.5 | GAN adversarial BCE |
| `diversity_weight` | **1.4** | Entropy + batch sim + pairwise |
| `entropy_weight` | 0.8 | Th√†nh ph·∫ßn entropy trong diversity |
| `ngram_weight` | 0.45 | Bigram + Trigram diversity |
| `length_penalty_weight` | 0.5 | EOS supervision |
| `reconstruction_weight` | 0.3 | MSE/CrossEntropy v·ªõi real sequences |
| `feature_matching_weight` | 0.2 | D feature matching |
| `stability_weight` | 0.1 | Stability bias loss |

---

## 11. K·∫øt qu·∫£ Th·ª±c nghi·ªám

### 11.1 K·∫øt qu·∫£ Training (Training Run #8 ‚Äî stable8)

**Hardware:** Quadro RTX 6000 (23.5 GB), CUDA
**Dataset:** 129,121 train / 27,669 val
**Resumed from:** epoch 187 (checkpoint_epoch_187.pt)

| Epoch | G Loss (val) | D Loss (val) | D‚ÄìG Gap | Entropy | N-gram | Len Pen |
|---|---|---|---|---|---|---|
| 189 | **0.4072** ‚≠ê | 2.0525 | 0.232 | 2.877 | 0.0100 | 0.0038 |
| 190 | 0.4128 | 2.0050 | 0.217 | 2.881 | 0.0097 | 0.0005 |
| 191 | 0.4132 | 2.0004 | 0.225 | 2.884 | 0.0097 | 0.0005 |
| 195 | 0.4137 | 1.9972 | 0.246 | 2.888 | 0.0094 | 0.0004 |
| 200 | 0.4137 | 1.9971 | 0.248 | 2.890 | 0.0091 | 0.0003 |
| 206 | 0.4136 | 1.9968 | 0.248 | 2.892 | 0.0078 | 0.0003 |

**Best checkpoint:** epoch 189, `val_g_loss = 0.4072`

### 11.2 ƒê√°nh gi√° theo M·ª•c ti√™u ƒê·ªÅ ra

| Metric | M·ª•c ti√™u | ƒê·∫°t ƒë∆∞·ª£c | Tr·∫°ng th√°i |
|---|---|---|---|
| G val loss | < 0.35 | **0.407** | üîÑ Ti·∫øp t·ª•c c·∫£i thi·ªán |
| D‚ÄìG gap | < 0.25 | **0.232** ‚úì | ‚úÖ ƒê·∫°t (epoch 189) |
| Entropy | > 2.80 | **2.892** | ‚úÖ ƒê·∫°t |
| N-gram diversity | > 0.08 | **0.0078** | üîÑ C√≤n th·∫•p |
| Length penalty | < 0.005 | **0.0003** | ‚úÖ ƒê·∫°t |

### 11.3 Th√¥ng s·ªë M√¥ h√¨nh

| Component | Parameters |
|---|---|
| Generator (GRUGenerator) | **2,392,344** |
| Discriminator (CNNDiscriminator) | **285,122** |
| **T·ªïng** | **2,677,466** |
| ESM2-650M (frozen evaluator) | 650,000,000 |

### 11.4 T·ªëc ƒë·ªô Training

```
Epoch time:    ~44 gi√¢y/epoch
Batches:       15 batches (129,121 / 8,192 ‚âà 15)
Throughput:    ~8,192 √ó 15 / 44 ‚âà 2,793 sequences/second
Checkpoint:    saved every 10 epochs (~7.3 ph√∫t/checkpoint)
```

---

## 12. C·∫•u tr√∫c D·ª± √°n

```
LightweightPeptideGen/
‚îú‚îÄ‚îÄ train.py                    # Entry point: training GAN
‚îú‚îÄ‚îÄ generate.py                 # Entry point: sinh peptide t·ª´ checkpoint
‚îú‚îÄ‚îÄ evaluate.py                 # Entry point: ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng
‚îÇ
‚îú‚îÄ‚îÄ peptidegen/                 # Core library package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py             # Public API exports
‚îÇ   ‚îú‚îÄ‚îÄ constants.py            # DIPEPTIDE_WEIGHTS, pKa tables, ...
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                # load_config(), set_seed(), ...
‚îÇ   ‚îú‚îÄ‚îÄ logger_config.py        # Logging setup
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/                   # Data pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vocabulary.py       # VOCAB (24 tokens), encode/decode
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py          # PeptideDataset, ConditionalPeptideDataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataloader.py       # create_dataloader()
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ features.py         # FeatureExtractor, normalization
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Neural network models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components.py       # PositionalEncoding, MultiHeadAttention, SelfAttention, ResidualBlock
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generator.py        # GRUGenerator, LSTMGenerator, TransformerGenerator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ discriminator.py    # CNNDiscriminator, RNNDiscriminator, HybridDiscriminator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ esm2_embedder.py    # ESM2Embedder, LightweightESMProjector, ESM2StructureEvaluator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ esm2_generator.py   # ESM2ConditionedGenerator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ structure_evaluator.py  # GraphAttentionLayer, LightweightGAT, StructureEvaluator
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_loss.py     # FeatureBasedLoss (stability + therapeutic + toxicity)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/               # Training logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py          # GANTrainer, ConditionalGANTrainer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ losses.py           # DiversityLoss, NgramDiversityLoss, LengthPenaltyLoss,
‚îÇ   ‚îÇ                           #   FeatureMatchingLoss, ReconstructionLoss, StabilityBiasLoss
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ inference/              # Generation/sampling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sampler.py          # PeptideSampler (from_checkpoint, sample, save_fasta)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/             # Evaluation metrics
‚îÇ       ‚îú‚îÄ‚îÄ stability.py        # calculate_instability_index, GRAVY, aliphatic_index, pI, ...
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py          # Diversity metrics + AMP metrics (merged from amp_metrics.py)
‚îÇ       ‚îî‚îÄ‚îÄ quality_filter.py   # QualityFilter pipeline
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml             # To√†n b·ªô hyperparameter config
‚îÇ
‚îú‚îÄ‚îÄ dataset/                    # Training data (FASTA + CSV)
‚îÇ   ‚îú‚îÄ‚îÄ train.fasta / train.csv
‚îÇ   ‚îú‚îÄ‚îÄ val.fasta / val.csv
‚îÇ   ‚îî‚îÄ‚îÄ test.fasta / test.csv
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/                # Saved model checkpoints (.pt files)
‚îú‚îÄ‚îÄ logs/                       # Training logs (train_stable8.log, ...)
‚îú‚îÄ‚îÄ tests/                      # Unit tests (pytest)
‚îÇ   ‚îú‚îÄ‚îÄ test_quick.py
‚îÇ   ‚îî‚îÄ‚îÄ test_conditional.py
‚îú‚îÄ‚îÄ tools/                      # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ process_data.py         # Data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ analyze_data.py         # Dataset analysis
‚îÇ   ‚îú‚îÄ‚îÄ validate_data.py        # Data validation
‚îÇ   ‚îî‚îÄ‚îÄ export.py               # Model export
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ environment.yml             # Conda environment spec
```

---

## 13. H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

### 13.1 C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng

```bash
conda env create -f environment.yml
conda activate peptidegen
```

### 13.2 Hu·∫•n luy·ªán

```bash
# Standard training
python train.py --config config/config.yaml

# Conditional training (s·ª≠ d·ª•ng 8 physicochemical features)
python train.py --config config/config.yaml --conditional

# Resume t·ª´ checkpoint
python train.py --config config/config.yaml --conditional \
    --resume checkpoints/checkpoint_epoch_189.pt \
    --epochs 500 \
    --fresh-optimizer           # reset Adam state n·∫øu c√≥ NaN

# V·ªõi custom hyperparameters
python train.py --config config/config.yaml --conditional \
    --epochs 200 --batch-size 4096 --lr 0.00003
```

### 13.3 Sinh Peptide

```bash
# Basic generation
python generate.py \
    --checkpoint checkpoints/best_model.pt \
    --num 1000 \
    --temperature 1.0 \
    --top-p 0.9

# V·ªõi quality filter
python generate.py \
    --checkpoint checkpoints/best_model.pt \
    --num 1000 \
    --filter \
    --min-stability 0.5 \
    --max-instability 40 \
    --output filtered_peptides.fasta

# Diverse generation
python generate.py \
    --checkpoint checkpoints/best_model.pt \
    --num 500 --diverse --temperature 0.8
```

### 13.4 ƒê√°nh gi√°

```bash
# ƒê√°nh gi√° c∆° b·∫£n
python evaluate.py --input generated.fasta

# So s√°nh v·ªõi training data
python evaluate.py \
    --input generated.fasta \
    --reference dataset/train.fasta \
    --output report.json
```

### 13.5 API Python

```python
from peptidegen import GANTrainer, ConditionalGANTrainer
from peptidegen import GRUGenerator, CNNDiscriminator
from peptidegen import PeptideSampler, VOCAB, load_config

# Load config
config = load_config('config/config.yaml')

# Build models
generator = GRUGenerator(
    vocab_size=24,
    embedding_dim=64,
    hidden_dim=256,
    latent_dim=128,
    num_layers=2,
    condition_dim=8,
    use_attention=True,
)
discriminator = CNNDiscriminator(
    vocab_size=24,
    embedding_dim=64,
    hidden_dim=256,
    num_filters=[64, 128, 256],
    kernel_sizes=[3, 5, 7],
    use_spectral_norm=True,
    use_minibatch_std=True,
)

# Train
trainer = ConditionalGANTrainer(generator, discriminator, config['training'])
trainer.fit(train_loader, val_loader, epochs=200, checkpoint_dir='checkpoints')

# Generate
sampler = PeptideSampler.from_checkpoint('checkpoints/best_model.pt')
sequences = sampler.sample(n=1000, temperature=0.8, top_p=0.9)
sampler.save_fasta(sequences, 'generated.fasta')
```

---

## 14. T√†i li·ªáu Tham kh·∫£o

### Ki·∫øn tr√∫c & K·ªπ thu·∫≠t GAN

1. **Goodfellow et al.** (2014). Generative Adversarial Nets. *NeurIPS*.
2. **Salimans et al.** (2016). Improved Techniques for Training GANs. *NeurIPS*. *(Feature Matching, Minibatch Discrimination)*
3. **Miyato et al.** (2018). Spectral Normalization for Generative Adversarial Networks. *ICLR*.
4. **Gulrajani et al.** (2017). Improved Training of Wasserstein GANs. *NeurIPS*.

### M√¥ h√¨nh Ng√¥n ng·ªØ Protein

5. **Lin et al.** (2022). Language models of protein sequences at the scale of evolution enable accurate structure prediction. *bioRxiv*. *(ESM2)*
6. **Rives et al.** (2021). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. *PNAS*. *(ESM)*

### Graph Neural Networks

7. **Veliƒçkoviƒá et al.** (2018). Graph Attention Networks. *ICLR*. *(GAT)*

### Sinh h√≥a Peptide

8. **Guruprasad et al.** (1990). Correlation between stability of a protein and its dipeptide composition. *Protein Engineering*. *(Instability Index)*
9. **Ikai** (1980). Thermostability and Aliphatic Index of Globular Proteins. *Journal of Biochemistry*. *(Aliphatic Index)*
10. **Kyte & Doolittle** (1982). A simple method for displaying the hydropathic character of a protein. *Journal of Molecular Biology*. *(GRAVY/Hydropathy)*
11. **Eisenberg et al.** (1982). Hydrophobic moments and protein structure. *Faraday Symposia*.

### Antimicrobial Peptides

12. **Boman** (2003). Antibacterial peptides: basic facts and emerging concepts. *Journal of Internal Medicine*.
13. **Hancock & Sahl** (2006). Antimicrobial and host-defense peptides as new anti-infective therapeutic strategies. *Nature Biotechnology*.

### Sinh t·∫°o Peptide v·ªõi Deep Learning

14. **Tucs et al.** (2023). Generating amphibian-inspired antimicrobial peptides with a recurrent neural network. *PLOS ONE*.
15. **Dean et al.** (2021). Deep learning for antimicrobial peptide discovery using generative models. *Briefings in Bioinformatics*.

---

## Appendix A: Checkpoint Format

```python
checkpoint = {
    'epoch': int,
    'generator_state_dict': dict,
    'discriminator_state_dict': dict,
    'g_optimizer_state_dict': dict,
    'd_optimizer_state_dict': dict,
    'scaler_state_dict': dict,      # AMP GradScaler
    'best_val_metric': float,
    'history': {
        'g_loss': [...],
        'd_loss': [...],
        'val_g_loss': [...],
        'val_d_loss': [...],
    },
    'model_config': {               # L∆∞u ki·∫øn tr√∫c ƒë·ªÉ reload ƒë√∫ng
        'vocab_size': 24,
        'embedding_dim': 64,
        'hidden_dim': 256,
        'latent_dim': 128,
        'max_length': 50,
        'num_layers': 2,
        'dropout': 0.2,
        'condition_dim': 8,
        'bidirectional': True,
        'use_attention': True,
        'pad_idx': 0,
        'sos_idx': 1,
        'eos_idx': 2,
    }
}
```

## Appendix B: Vocabulary

```
Index 0: <PAD>   ‚Äî padding token
Index 1: <SOS>   ‚Äî start of sequence
Index 2: <EOS>   ‚Äî end of sequence
Index 3: <UNK>   ‚Äî unknown amino acid
Index 4‚Äì23: A C D E F G H I K L M N P Q R S T V W Y
             (20 standard amino acids, alphabetical)
```

---

*T√†i li·ªáu n√†y ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ ph√¢n t√≠ch to√†n b·ªô m√£ ngu·ªìn LightweightPeptideGen. C·∫≠p nh·∫≠t l·∫ßn cu·ªëi: Th√°ng 2/2026.*
